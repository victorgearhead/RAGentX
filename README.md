# RAGentX: A Nextâ€‘Gen Multiâ€‘Agent RAG Platform

**RAGentX** fuses **Hybrid Retrieval**, **Knowledge Graphs**, and **Advanced LLM Reasoning** into an end-to-end contextâ€‘aware AI agentâ€”optimised for enterpriseâ€‘scale, complex unstructured data.

ğŸ¤– AI-Powered Document Processing:

- Automatic Question Extraction: Upload documents and automatically extract structured questions
- Intelligent Response Generation: Generate contextual responses using your organization's documents
- Multi-Step AI Analysis: Advanced reasoning process that analyzes, searches(dense and sparse), extracts, and synthesizes responses
- Document Understanding: Supports Word, PDF, Excel, and Docs.
---

## ğŸ¯ Elevator Pitch

RAGentX is a ground-breaking **multi-agent** framework that delivers:

- **Adapterâ€‘Free Hybrid RAG (BM42 + Dense)** via **Qdrant**, blending highâ€‘precision BM25 matches with semantic embeddings for lightningâ€‘fast recall.
- **Graphâ€‘RAG**: Leverages **Neo4j** to traverse multi-hop entity relationships, turning data silos into interconnected knowledge webs.
- **Chainâ€‘ofâ€‘Thought (CoT) & ReAct**: Empowers LLMs to _think aloud_, plan actions, call retrieval or graph queries, and dynamically refine answers.
- **PEDRO/LoRA Fineâ€‘Tuning**: Efficiently adapts QA models on your domain via **PEFT**, using autoâ€‘generated questionâ€‘answer pairs.
- **Selfâ€‘RAG Selfâ€‘Critique**: Runs a secondary verification passâ€”retrieves evidence, distills context, and critiques its own output to eliminate hallucinations.
- **Contextual Awareness**: Sessionâ€‘scoped cache, onâ€‘theâ€‘fly distillation, and persistent config empower seamless multiâ€‘turn insights.
- **Possible Future Updates**: For enterprise scale and more accurate results, it is suggested to use a context window instead of sparse retrieval and also is suggestable to use neo4j graphs for cross-doc links in context windows. This can bring alot of change in retrieval 
---

---

## ğŸ” Why  RAGentX Outperforms Traditional RAG Systems

1. **Truly Hybrid Retrieval (BM42 + Dense)**  
   - **Old way:** BM25 alone misses semantic matches; pure vector search can hallucinate or miss keywords.  
   - **RAGentX:** Combines BM25 and dense embeddings in Qdrant for bestâ€‘practice recall & precision.
2. **Graphâ€‘RAG for Multiâ€‘Hop Reasoning**  
   - **Old way:** Flat document retrieval canâ€™t answer chained queries.  
   - **RAGentX:** Utilizes Neo4j graph traversal to enable true multiâ€‘hop inferencing across entities.
3. **ReAct + Chainâ€‘ofâ€‘Thought (CoT) Orchestration**  
   - **Old way:** Singleâ€‘shot LLM prompts produce brittle, hallucinationâ€‘prone answers.  
   - **RAGentX:** A multiâ€‘tool ReAct loop (_Retrieve, Distill, Graph, LLM, Critique_) yields structured, traceable reasoning.
4. **PEDRO/LoRA Fineâ€‘Tuned QA**  
   - **Old way:** Fullâ€‘model retraining is costly and timeâ€‘consuming.  
   - **RAGentX:** PEFT/LoRA adapters adapt in minutes on autoâ€‘generated QA pairs, delivering domainâ€‘specific accuracy without heavy compute.
5. **Selfâ€‘RAG Selfâ€‘Critique**  
   - **Old way:** Answers often lack provenance or verification.  
   - **RAGentX:** Automatically retrieves evidence, distills context, and prompts the model to critique its own output, drastically reducing hallucinations.
6. **Persistent Session Cache & Dynamic Config**  
   - **Old way:** Rebuilding indices per run loses workspace context.  
   - **RAGentX:** Maintains a durable session directory that survives restarts and allows onâ€‘theâ€‘fly hyperparameter tuning via Streamlit.

---

## ğŸ— Pipeline Breakdown

1. **Ingestion & Chunking**  
   - **LlamaIndex** ingests PDFs, CSVs, Excels, TXTs â†’ overlapping text chunks (configurable size/overlap).  
   - **Ollama LLM** autoâ€‘generates QA pairs from each chunk for fineâ€‘tuning.
2. **Hybrid Retrieval**  
   - **BM25** index over tokenized chunks + **dense embeddings** in Qdrant â†’ unified candidate set.
3. **Knowledge Graph Construction**  
   - **spaCy NER** extracts entities â†’ **Neo4j** MERGE stores nodes & relations.  
   - Supports multiâ€‘hop **Cypher** queries for entityâ€‘centric inferrence.
4. **QA Fineâ€‘Tuning (PEDRO/LoRA)**  
   - Converts autoâ€‘generated QA pairs into a **PEFT** adapter on any HuggingFace QA model.  
   - Outputs a lightweight, domainâ€‘tuned QA head for precise answers.
5. **ReAct Agent & Selfâ€‘Critique**  
   - **Multiâ€‘step loop:** Retrieve â†’ Distill â†’ Graph â†’ LLM Answer â†’ Selfâ€‘Critique.  
   - **New facts** from answers are reâ€‘ingested into the KG for evolving context.
6. **API & Frontend**  
   - **FastAPI** serves session, ingest, and query endpoints.  
   - **Streamlit** UI for file uploads, parameter tuning, and interactive Q&A with colorâ€‘coded responses and logs.

---

## ğŸŒŸ Uniqueness & Stateâ€‘ofâ€‘theâ€‘Art

- **Holistic integration** of sparse & dense retrieval, graph reasoning, and selfâ€‘verifying ReAct planningâ€”no other openâ€‘source platform ties all these together.  
- **Realâ€‘time LoRA adapters** via PEDRO remove the need for largeâ€‘scale retraining, making custom QA immediate.  
- **Selfâ€‘critique loop** ensures each answer is backed by distilled evidence, inspired by Selfâ€‘RAG research (Korshunova et al., 2023).  
- **Runtimeâ€‘configurable** via Streamlit: nonâ€‘engineers can tweak models, hyperparameters, and reasoning modes without code changes.  
- **Extensible**: swap out LLM providers (Ollama, OpenAI, HF), embed models, or graph backends with minimal code adjustments.

---

## ğŸ”§ Tech Stack & Components

| Feature                               | Description                                                         | Reference                                |
|---------------------------------------|----------------------------------------------------------------------|------------------------------------------|
| **Hybrid BM42 + Dense**               | BM25 enhanced with vector search in Qdrant.                          | Qdrant BM42 blog                        |
| **Knowledge Graph (KG)**              | Neo4j-based entity & relation store with multi-hop Cypher queries.   | RAG (Lewis et al., 2020)                |
| **Chainâ€‘ofâ€‘Thought (CoT)**            | Stepwise reasoning prompts inside ReAct agent.                        | Wei et al., 2022                         |
| **ReAct Agent**                       | Planning & acting loop: retrieve, distill, critique, graph, answer.   | Yao et al., 2023                         |
| **PEDRO/LoRA Fineâ€‘Tuning**            | Parameterâ€‘efficient domain adaptation of QA models.                  | [PEDRO preprint, 2024]                  |
| **Selfâ€‘Critique (Selfâ€‘RAG)**          | Automatic answer verification against retrieved evidence.             | Korshunova et al., 2023                  |
| **Planâ€‘andâ€‘Solve Prompting**          | Ongoing LLMâ€‘driven plan refinement.                                  | Shen et al., 2023                        |
| **Session Cache & Context**           | Durable session directory, clear on user command.                    | â€”                                        |

---

## ğŸ— Architecture Diagram

```mermaid
flowchart LR
  A[File Ingestion] -->|Chunk| B((Chunks))
  B --> C[Auto QAâ€‘Pair Gen]
  B --> D[Entity Extraction]
  D --> E[Neo4j KG]

  F[User Query] --> G[BM42 + Dense]
  G --> H[Candidate Chunks]
  H --> I[ReAct Planner]
  I -->|Action: Graph| E
  I -->|Action: Distill| J[Distilled Summary]
  I -->|Action: LLM| K[LLM Answer]
  K --> L[Selfâ€‘Critique]
  L --> E
  L --> M[Final Answer]
```

RAGentX : https://huggingface.co/spaces/VictorGearhead/RAGentX (Runs on 16GB VRAM, might be slow)
(suggested to refer for further debugged code and more features)

## âš™ï¸ Configuration & Streamlit Setup

All settings live in `config.py` (or can be adjusted at runtime via the Streamlit sidebar). No manual `.env` edits requiredâ€”just start the Streamlit UI and set:

- **Qdrant URL**
- **Neo4j URI/User/Pass**
- **Embedding Model**
- **LLM Provider & Model & Key**
- **Session Directory**
- **Chunk Size & Overlap**
- **LoRA Hyperparams & PEFT settings**
- **Fineâ€‘tuning params (epochs, batch size, LR)**

---

## ğŸ–¥ï¸ Quickstart

```bash
cd RAGentX
docker-compose up --build
```

â€¢ **Start Session** â†’ **Ingest** â†’ **Ask Questions** â†’ **End Session**  
â€¢ Toggle **Fallback** and **Think** in UI for LLM fallback & CoT  
â€¢ Upload multiple files at once  
â€¢ View answers, distilled summaries, and selfâ€‘critique in distinct colored panels

---

## ğŸ“– Citations

1. _Retrievalâ€‘Augmented Generation_ (RAG), Lewis et al., 2020.  
2. _Selfâ€‘RAG: Selfâ€‘Critique in Retrievalâ€‘Augmented Generation_, Korshunova et al., 2023.  
3. _ReAct: Reason + Act in Language Models_, Yao et al., 2023.  
4. _Planâ€‘andâ€‘Solve Prompting_, Shen et al., 2023.  
5. _PEDRO: PEFT for QA Task Adaptation_, 2024 preprint.  
6. _BM42: BM25 + Vector Search_, Qdrant Tech Blog.  
7. _Multiâ€‘Agent RAG Orchestration_, Rayo et al., 2025.

---
